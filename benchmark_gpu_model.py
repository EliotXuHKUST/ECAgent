#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GPUæ¨¡å‹æ€§èƒ½åŸºå‡†æµ‹è¯•
æµ‹è¯•è®­ç»ƒåæ¨¡å‹çš„æ¨ç†æ€§èƒ½å’Œè´¨é‡
"""

import torch
import time
import sys
from pathlib import Path
from typing import List, Dict, Any
import json

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

def check_model_exists(model_path: str) -> bool:
    """æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨"""
    model_dir = Path(model_path)
    if not model_dir.exists():
        print(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        return False
    
    # æ£€æŸ¥å¿…è¦æ–‡ä»¶
    required_files = ["config.json", "pytorch_model.bin", "tokenizer.json"]
    missing_files = []
    
    for file_name in required_files:
        if not (model_dir / file_name).exists():
            missing_files.append(file_name)
    
    if missing_files:
        print(f"âš ï¸  æ¨¡å‹æ–‡ä»¶ä¸å®Œæ•´ï¼Œç¼ºå°‘: {missing_files}")
        print("å°è¯•åŠ è½½åŸºç¡€æ¨¡å‹è¿›è¡Œæµ‹è¯•...")
    
    return True

def load_model_for_benchmark(model_path: str):
    """åŠ è½½æ¨¡å‹ç”¨äºåŸºå‡†æµ‹è¯•"""
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM
        
        print(f"ğŸ“¦ åŠ è½½æ¨¡å‹: {model_path}")
        
        # åŠ è½½tokenizer
        tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True,
            padding_side='left'
        )
        
        # è®¾ç½®pad_token
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # åŠ è½½æ¨¡å‹
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto" if torch.cuda.is_available() else None,
            trust_remote_code=True
        )
        
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        print(f"   è®¾å¤‡: {next(model.parameters()).device}")
        print(f"   å‚æ•°é‡: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M")
        
        return tokenizer, model
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None, None

def benchmark_inference_speed(tokenizer, model, test_queries: List[str], num_runs: int = 3):
    """åŸºå‡†æµ‹è¯•æ¨ç†é€Ÿåº¦"""
    print(f"\nğŸƒ æ¨ç†é€Ÿåº¦æµ‹è¯• (è¿è¡Œ{num_runs}æ¬¡å–å¹³å‡å€¼)")
    print("-" * 60)
    
    results = []
    
    for query in test_queries:
        query_times = []
        responses = []
        
        for run in range(num_runs):
            start_time = time.time()
            
            # å‡†å¤‡è¾“å…¥
            prompt = f"ç”¨æˆ·ï¼š{query}\nå®¢æœï¼š"
            inputs = tokenizer(prompt, return_tensors="pt")
            
            if torch.cuda.is_available():
                inputs = {k: v.to(model.device) for k, v in inputs.items()}
            
            # ç”Ÿæˆå›å¤
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=100,
                    temperature=0.1,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id,
                    repetition_penalty=1.1
                )
            
            # è§£ç å›å¤
            response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            response = response.replace(prompt, "").strip()
            
            end_time = time.time()
            inference_time = end_time - start_time
            
            query_times.append(inference_time)
            if run == 0:  # åªä¿å­˜ç¬¬ä¸€æ¬¡çš„å›å¤
                responses.append(response)
        
        avg_time = sum(query_times) / len(query_times)
        
        results.append({
            'query': query,
            'avg_time': avg_time,
            'response': responses[0] if responses else ""
        })
        
        print(f"Query: {query}")
        print(f"å¹³å‡æ—¶é—´: {avg_time:.2f}s")
        print(f"Response: {responses[0][:100]}..." if responses and len(responses[0]) > 100 else f"Response: {responses[0] if responses else 'N/A'}")
        print("-" * 60)
    
    # è®¡ç®—æ€»ä½“ç»Ÿè®¡
    total_avg_time = sum(r['avg_time'] for r in results) / len(results)
    throughput = 1 / total_avg_time
    
    print(f"\nğŸ“Š æ€»ä½“æ€§èƒ½:")
    print(f"   å¹³å‡æ¨ç†æ—¶é—´: {total_avg_time:.2f}s")
    print(f"   ååé‡: {throughput:.2f} queries/second")
    
    return results

def benchmark_gpu_memory(model):
    """åŸºå‡†æµ‹è¯•GPUå†…å­˜ä½¿ç”¨"""
    if not torch.cuda.is_available():
        print("\nâš ï¸  GPUä¸å¯ç”¨ï¼Œè·³è¿‡å†…å­˜æµ‹è¯•")
        return
    
    print(f"\nğŸ’¾ GPUå†…å­˜ä½¿ç”¨æµ‹è¯•")
    print("-" * 40)
    
    for i in range(torch.cuda.device_count()):
        gpu_name = torch.cuda.get_device_name(i)
        memory_allocated = torch.cuda.memory_allocated(i) / 1e9
        memory_reserved = torch.cuda.memory_reserved(i) / 1e9
        memory_total = torch.cuda.get_device_properties(i).total_memory / 1e9
        
        print(f"GPU {i} ({gpu_name}):")
        print(f"   å·²åˆ†é…å†…å­˜: {memory_allocated:.2f}GB")
        print(f"   é¢„ç•™å†…å­˜: {memory_reserved:.2f}GB")
        print(f"   æ€»å†…å­˜: {memory_total:.2f}GB")
        print(f"   ä½¿ç”¨ç‡: {(memory_allocated/memory_total)*100:.1f}%")

def benchmark_model_quality(results: List[Dict[str, Any]]):
    """åŸºå‡†æµ‹è¯•æ¨¡å‹å›å¤è´¨é‡"""
    print(f"\nğŸ¯ æ¨¡å‹è´¨é‡è¯„ä¼°")
    print("-" * 40)
    
    # ç®€å•çš„è´¨é‡æŒ‡æ ‡
    quality_metrics = {
        'avg_response_length': 0,
        'responses_with_greeting': 0,
        'responses_with_closing': 0,
        'empty_responses': 0
    }
    
    for result in results:
        response = result['response']
        
        # å“åº”é•¿åº¦
        quality_metrics['avg_response_length'] += len(response)
        
        # åŒ…å«é—®å€™è¯­
        if any(greeting in response for greeting in ['æ‚¨å¥½', 'ä½ å¥½', 'æ¬¢è¿']):
            quality_metrics['responses_with_greeting'] += 1
        
        # åŒ…å«ç»“æŸè¯­
        if any(closing in response for closing in ['è°¢è°¢', 'æ„Ÿè°¢', 'è¿˜æœ‰å…¶ä»–é—®é¢˜', 'éšæ—¶å’¨è¯¢']):
            quality_metrics['responses_with_closing'] += 1
        
        # ç©ºå“åº”
        if not response.strip():
            quality_metrics['empty_responses'] += 1
    
    # è®¡ç®—å¹³å‡å€¼å’Œç™¾åˆ†æ¯”
    num_results = len(results)
    quality_metrics['avg_response_length'] /= num_results
    
    print(f"å¹³å‡å›å¤é•¿åº¦: {quality_metrics['avg_response_length']:.1f} å­—ç¬¦")
    print(f"åŒ…å«é—®å€™è¯­: {quality_metrics['responses_with_greeting']}/{num_results} ({quality_metrics['responses_with_greeting']/num_results*100:.1f}%)")
    print(f"åŒ…å«ç»“æŸè¯­: {quality_metrics['responses_with_closing']}/{num_results} ({quality_metrics['responses_with_closing']/num_results*100:.1f}%)")
    print(f"ç©ºå›å¤: {quality_metrics['empty_responses']}/{num_results} ({quality_metrics['empty_responses']/num_results*100:.1f}%)")
    
    return quality_metrics

def save_benchmark_results(results: List[Dict[str, Any]], quality_metrics: Dict[str, Any], output_file: str = "benchmark_results.json"):
    """ä¿å­˜åŸºå‡†æµ‹è¯•ç»“æœ"""
    benchmark_data = {
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
        'performance_results': results,
        'quality_metrics': quality_metrics,
        'system_info': {
            'cuda_available': torch.cuda.is_available(),
            'gpu_count': torch.cuda.device_count() if torch.cuda.is_available() else 0,
            'pytorch_version': torch.__version__
        }
    }
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(benchmark_data, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ åŸºå‡†æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="ECAgent GPUæ¨¡å‹æ€§èƒ½åŸºå‡†æµ‹è¯•")
    parser.add_argument("--model_path", "-m", type=str, default="./models/gpu_fine_tuned",
                       help="æ¨¡å‹è·¯å¾„")
    parser.add_argument("--fallback_model", type=str, default="Qwen/Qwen-7B-Chat",
                       help="å¤‡ç”¨æ¨¡å‹åç§°")
    parser.add_argument("--runs", "-r", type=int, default=3,
                       help="æ¯ä¸ªæµ‹è¯•è¿è¡Œæ¬¡æ•°")
    parser.add_argument("--output", "-o", type=str, default="benchmark_results.json",
                       help="ç»“æœè¾“å‡ºæ–‡ä»¶")
    
    args = parser.parse_args()
    
    print("ğŸ§ª ECAgent GPUæ¨¡å‹æ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("=" * 50)
    
    # æµ‹è¯•ç”¨ä¾‹
    test_queries = [
        "å¦‚ä½•é€€è´§ï¼Ÿ",
        "è¿™ä¸ªå•†å“æœ‰ä¿ä¿®å—ï¼Ÿ",
        "æ”¯æŒè´§åˆ°ä»˜æ¬¾å—ï¼Ÿ",
        "å¿«é€’å¤šä¹…èƒ½åˆ°ï¼Ÿ",
        "å¯ä»¥ä½¿ç”¨ä¼˜æƒ åˆ¸å—ï¼Ÿ",
        "å•†å“ç¼ºè´§æ€ä¹ˆåŠï¼Ÿ",
        "å¦‚ä½•ç”³è¯·å”®åæœåŠ¡ï¼Ÿ",
        "é…é€èŒƒå›´åŒ…æ‹¬å“ªäº›åœ°åŒºï¼Ÿ"
    ]
    
    # æ£€æŸ¥æ¨¡å‹
    model_path = args.model_path
    if not check_model_exists(model_path):
        print(f"ä½¿ç”¨å¤‡ç”¨æ¨¡å‹: {args.fallback_model}")
        model_path = args.fallback_model
    
    # åŠ è½½æ¨¡å‹
    tokenizer, model = load_model_for_benchmark(model_path)
    if tokenizer is None or model is None:
        print("âŒ æ— æ³•åŠ è½½æ¨¡å‹ï¼ŒåŸºå‡†æµ‹è¯•å¤±è´¥")
        return
    
    # è¿è¡ŒåŸºå‡†æµ‹è¯•
    try:
        # æ¨ç†é€Ÿåº¦æµ‹è¯•
        results = benchmark_inference_speed(tokenizer, model, test_queries, args.runs)
        
        # GPUå†…å­˜æµ‹è¯•
        benchmark_gpu_memory(model)
        
        # æ¨¡å‹è´¨é‡è¯„ä¼°
        quality_metrics = benchmark_model_quality(results)
        
        # ä¿å­˜ç»“æœ
        save_benchmark_results(results, quality_metrics, args.output)
        
        print(f"\nâœ… åŸºå‡†æµ‹è¯•å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ åŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
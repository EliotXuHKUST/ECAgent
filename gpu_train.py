#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GPUç¯å¢ƒä¸‹çš„ECAgentæ¨¡å‹è®­ç»ƒè„šæœ¬
æ”¯æŒQLoRAé‡åŒ–è®­ç»ƒå’Œå¤šGPUåˆ†å¸ƒå¼è®­ç»ƒ
"""

import os
import sys
from pathlib import Path
import torch

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

def update_env_config():
    """æ›´æ–°ç¯å¢ƒé…ç½®ä¸ºGPUæ¨¡å¼"""
    os.environ.update({
        "USE_GPU": "True",
        "CUDA_VISIBLE_DEVICES": "0",  # æŒ‡å®šä½¿ç”¨çš„GPU
        "MAX_SAMPLES": "500",  # GPUç¯å¢ƒå¯ä»¥å¤„ç†æ›´å¤šæ ·æœ¬
        "MAX_EPOCHS": "3",
        "BATCH_SIZE": "4",  # GPUå¯ä»¥ä½¿ç”¨æ›´å¤§çš„æ‰¹æ¬¡å¤§å°
        "LEARNING_RATE": "2e-4",
        "USE_QUANTIZATION": "True",  # å¯ç”¨4bité‡åŒ–
        "GRADIENT_CHECKPOINTING": "True",  # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
        "DATALOADER_NUM_WORKERS": "4"
    })

def check_gpu_environment():
    """æ£€æŸ¥GPUç¯å¢ƒ"""
    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨ï¼è¯·æ£€æŸ¥GPUé©±åŠ¨å’ŒPyTorchå®‰è£…")
        return False
    
    gpu_count = torch.cuda.device_count()
    print(f"âœ… æ£€æµ‹åˆ° {gpu_count} ä¸ªGPUè®¾å¤‡")
    
    for i in range(gpu_count):
        gpu_name = torch.cuda.get_device_name(i)
        gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1e9
        print(f"   GPU {i}: {gpu_name} ({gpu_memory:.1f}GB)")
    
    return True

def optimize_gpu_settings():
    """ä¼˜åŒ–GPUæ€§èƒ½è®¾ç½®"""
    # GPUå†…å­˜ä¼˜åŒ–è®¾ç½®
    torch.backends.cudnn.benchmark = True  # ä¼˜åŒ–å·ç§¯æ€§èƒ½
    torch.backends.cuda.matmul.allow_tf32 = True  # å¯ç”¨TF32
    torch.backends.cudnn.allow_tf32 = True
    
    print("âœ… GPUæ€§èƒ½ä¼˜åŒ–è®¾ç½®å·²å¯ç”¨")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹ECAgent GPUè®­ç»ƒæ¨¡å¼...")
    
    # æ£€æŸ¥GPUç¯å¢ƒ
    if not check_gpu_environment():
        return
    
    # ä¼˜åŒ–GPUè®¾ç½®
    optimize_gpu_settings()
    
    # æ›´æ–°ç¯å¢ƒé…ç½®
    update_env_config()
    
    try:
        from train_ecommerce_faq import load_and_process_ecommerce_faq, train_ecommerce_model
        
        # åŠ è½½å’Œå¤„ç†æ•°æ®
        print("ğŸ“Š åŠ è½½å’Œå¤„ç†æ•°æ®...")
        data_path = load_and_process_ecommerce_faq(
            output_path="./data/ecommerce_faq_gpu_train.json",
            max_samples=int(os.environ.get("MAX_SAMPLES", 500))
        )
        
        # å¼€å§‹GPUè®­ç»ƒ
        print("ğŸ¯ å¼€å§‹GPUæ¨¡å‹è®­ç»ƒ...")
        print(f"ğŸ“‹ è®­ç»ƒé…ç½®:")
        print(f"   - æ ·æœ¬æ•°é‡: {os.environ.get('MAX_SAMPLES')}")
        print(f"   - è®­ç»ƒè½®æ¬¡: {os.environ.get('MAX_EPOCHS')}")
        print(f"   - æ‰¹æ¬¡å¤§å°: {os.environ.get('BATCH_SIZE')}")
        print(f"   - å­¦ä¹ ç‡: {os.environ.get('LEARNING_RATE')}")
        print(f"   - é‡åŒ–è®­ç»ƒ: {os.environ.get('USE_QUANTIZATION')}")
        
        model_path = train_ecommerce_model(
            data_path=data_path,
            output_dir="./models/gpu_fine_tuned",
            num_epochs=int(os.environ.get("MAX_EPOCHS", 3)),
            batch_size=int(os.environ.get("BATCH_SIZE", 4)),
            learning_rate=float(os.environ.get("LEARNING_RATE", 2e-4)),
            use_quantization=os.environ.get("USE_QUANTIZATION", "True").lower() == "true"
        )
        
        print(f"âœ… GPUè®­ç»ƒå®Œæˆï¼")
        print(f"ğŸ“ æ¨¡å‹ä¿å­˜è·¯å¾„: {model_path}")
        print("\nğŸ§ª æµ‹è¯•å»ºè®®:")
        print("1. è¿è¡Œå‰ç«¯æµ‹è¯•: python simple_gradio_test.py")
        print("2. æ€§èƒ½åŸºå‡†æµ‹è¯•: python benchmark_gpu_model.py")
        print("3. æ¨¡å‹è´¨é‡è¯„ä¼°: python evaluate_model.py")
        print("4. GPUç›‘æ§: python monitor_gpu.py")
        
    except Exception as e:
        print(f"âŒ GPUè®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
# ECAgent GPUç¯å¢ƒéƒ¨ç½²æŒ‡å—

## ğŸ“‹ ç›®å½•
- [ç³»ç»Ÿè¦æ±‚](#ç³»ç»Ÿè¦æ±‚)
- [ç¯å¢ƒå‡†å¤‡](#ç¯å¢ƒå‡†å¤‡)
- [ä¾èµ–å®‰è£…](#ä¾èµ–å®‰è£…)
- [GPUé…ç½®](#gpué…ç½®)
- [æ¨¡å‹è®­ç»ƒ](#æ¨¡å‹è®­ç»ƒ)
- [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
- [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

## ğŸ”§ ç³»ç»Ÿè¦æ±‚

### ç¡¬ä»¶è¦æ±‚
- **GPU**: NVIDIA GPU (æ¨è RTX 3090/4090, A100, V100 ç­‰)
- **æ˜¾å­˜**: 
  - æœ€ä½è¦æ±‚: 12GB (7Bæ¨¡å‹ + 4bité‡åŒ–)
  - æ¨èé…ç½®: 24GB+ (7Bæ¨¡å‹å…¨ç²¾åº¦è®­ç»ƒ)
  - ç†æƒ³é…ç½®: 40GB+ (æ”¯æŒæ›´å¤§æ‰¹æ¬¡å’Œæ›´å¤æ‚æ¨¡å‹)
- **å†…å­˜**: 32GB+ ç³»ç»Ÿå†…å­˜
- **å­˜å‚¨**: 100GB+ å¯ç”¨ç©ºé—´

### è½¯ä»¶è¦æ±‚
- **æ“ä½œç³»ç»Ÿ**: Linux (Ubuntu 20.04+) / Windows 10+ / macOS
- **Python**: 3.8 - 3.11
- **CUDA**: 11.8+ æˆ– 12.x
- **Docker**: å¯é€‰ï¼Œç”¨äºå®¹å™¨åŒ–éƒ¨ç½²

## ğŸš€ ç¯å¢ƒå‡†å¤‡

### 1. CUDAå®‰è£…éªŒè¯
```bash
# æ£€æŸ¥CUDAç‰ˆæœ¬
nvcc --version
nvidia-smi

# éªŒè¯PyTorch CUDAæ”¯æŒ
python -c "import torch; print(f'CUDA Available: {torch.cuda.is_available()}')"
python -c "import torch; print(f'CUDA Version: {torch.version.cuda}')"
python -c "import torch; print(f'GPU Count: {torch.cuda.device_count()}')"
```

### 2. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv ecagent_gpu_env
source ecagent_gpu_env/bin/activate  # Linux/macOS
# æˆ– ecagent_gpu_env\Scripts\activate  # Windows

# å‡çº§pip
pip install --upgrade pip
```

## ğŸ“¦ ä¾èµ–å®‰è£…

### 1. å®‰è£…PyTorch (GPUç‰ˆæœ¬)
```bash
# CUDA 11.8
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# CUDA 12.1
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# éªŒè¯å®‰è£…
python -c "import torch; print(torch.cuda.is_available())"
```

### 2. å®‰è£…é¡¹ç›®ä¾èµ–
```bash
# å®‰è£…åŸºç¡€ä¾èµ–
pip install -r requirements.txt

# å®‰è£…é¢å¤–GPUä¼˜åŒ–ä¾èµ–
pip install flash-attn --no-build-isolation  # å¯é€‰ï¼Œæå‡æ³¨æ„åŠ›è®¡ç®—æ•ˆç‡
pip install deepspeed  # å¯é€‰ï¼Œæ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒ
pip install ninja  # ç¼–è¯‘åŠ é€Ÿ
```

### 3. éªŒè¯å…³é”®ç»„ä»¶
```bash
# éªŒè¯transformers
python -c "from transformers import AutoTokenizer; print('Transformers OK')"

# éªŒè¯PEFT
python -c "from peft import LoraConfig; print('PEFT OK')"

# éªŒè¯bitsandbytes
python -c "import bitsandbytes as bnb; print('Bitsandbytes OK')"

# éªŒè¯æ•°æ®é›†åŠ è½½
python -c "from datasets import load_dataset; print('Datasets OK')"
```

## âš™ï¸ GPUé…ç½®

### 1. åˆ›å»ºGPUè®­ç»ƒè„šæœ¬
```bash
# å¤åˆ¶å¹¶ä¿®æ”¹å¿«é€Ÿè®­ç»ƒè„šæœ¬
cp quick_train.py gpu_train.py
```

### 2. GPUè®­ç»ƒé…ç½®æ–‡ä»¶
åˆ›å»º `gpu_train.py`:

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GPUç¯å¢ƒä¸‹çš„ECAgentæ¨¡å‹è®­ç»ƒè„šæœ¬
æ”¯æŒQLoRAé‡åŒ–è®­ç»ƒå’Œå¤šGPUåˆ†å¸ƒå¼è®­ç»ƒ
"""

import os
import sys
from pathlib import Path
import torch

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

def update_env_config():
    """æ›´æ–°ç¯å¢ƒé…ç½®ä¸ºGPUæ¨¡å¼"""
    os.environ.update({
        "USE_GPU": "True",
        "CUDA_VISIBLE_DEVICES": "0",  # æŒ‡å®šä½¿ç”¨çš„GPU
        "MAX_SAMPLES": "500",  # GPUç¯å¢ƒå¯ä»¥å¤„ç†æ›´å¤šæ ·æœ¬
        "MAX_EPOCHS": "3",
        "BATCH_SIZE": "4",  # GPUå¯ä»¥ä½¿ç”¨æ›´å¤§çš„æ‰¹æ¬¡å¤§å°
        "LEARNING_RATE": "2e-4",
        "USE_QUANTIZATION": "True",  # å¯ç”¨4bité‡åŒ–
        "GRADIENT_CHECKPOINTING": "True",  # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
        "DATALOADER_NUM_WORKERS": "4"
    })

def check_gpu_environment():
    """æ£€æŸ¥GPUç¯å¢ƒ"""
    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨ï¼è¯·æ£€æŸ¥GPUé©±åŠ¨å’ŒPyTorchå®‰è£…")
        return False
    
    gpu_count = torch.cuda.device_count()
    print(f"âœ… æ£€æµ‹åˆ° {gpu_count} ä¸ªGPUè®¾å¤‡")
    
    for i in range(gpu_count):
        gpu_name = torch.cuda.get_device_name(i)
        gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1e9
        print(f"   GPU {i}: {gpu_name} ({gpu_memory:.1f}GB)")
    
    return True

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹ECAgent GPUè®­ç»ƒæ¨¡å¼...")
    
    # æ£€æŸ¥GPUç¯å¢ƒ
    if not check_gpu_environment():
        return
    
    # æ›´æ–°ç¯å¢ƒé…ç½®
    update_env_config()
    
    try:
        from train_ecommerce_faq import load_and_process_ecommerce_faq, train_ecommerce_model
        
        # åŠ è½½å’Œå¤„ç†æ•°æ®
        print("ğŸ“Š åŠ è½½å’Œå¤„ç†æ•°æ®...")
        data_path = load_and_process_ecommerce_faq(
            output_path="./data/ecommerce_faq_gpu_train.json",
            max_samples=int(os.environ.get("MAX_SAMPLES", 500))
        )
        
        # å¼€å§‹GPUè®­ç»ƒ
        print("ğŸ¯ å¼€å§‹GPUæ¨¡å‹è®­ç»ƒ...")
        model_path = train_ecommerce_model(
            data_path=data_path,
            output_dir="./models/gpu_fine_tuned",
            num_epochs=int(os.environ.get("MAX_EPOCHS", 3)),
            batch_size=int(os.environ.get("BATCH_SIZE", 4)),
            learning_rate=float(os.environ.get("LEARNING_RATE", 2e-4)),
            use_quantization=os.environ.get("USE_QUANTIZATION", "True").lower() == "true"
        )
        
        print(f"âœ… GPUè®­ç»ƒå®Œæˆï¼")
        print(f"ğŸ“ æ¨¡å‹ä¿å­˜è·¯å¾„: {model_path}")
        print("\nğŸ§ª æµ‹è¯•å»ºè®®:")
        print("1. è¿è¡Œå‰ç«¯æµ‹è¯•: python simple_gradio_test.py")
        print("2. æ€§èƒ½åŸºå‡†æµ‹è¯•: python benchmark_gpu_model.py")
        print("3. æ¨¡å‹è´¨é‡è¯„ä¼°: python evaluate_model.py")
        
    except Exception as e:
        print(f"âŒ GPUè®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
```

### 3. å¤šGPUé…ç½® (å¯é€‰)
å¯¹äºå¤šGPUç¯å¢ƒï¼Œåˆ›å»º `multi_gpu_train.py`:

```python
#!/usr/bin/env python3
"""
å¤šGPUåˆ†å¸ƒå¼è®­ç»ƒè„šæœ¬
"""
import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
import os

def setup_distributed():
    """è®¾ç½®åˆ†å¸ƒå¼è®­ç»ƒ"""
    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:
        rank = int(os.environ["RANK"])
        world_size = int(os.environ['WORLD_SIZE'])
        gpu = int(os.environ['LOCAL_RANK'])
    else:
        print('Not using distributed mode')
        return False, 0, 1, 0

    torch.cuda.set_device(gpu)
    dist.init_process_group(backend='nccl', rank=rank, world_size=world_size)
    dist.barrier()
    return True, rank, world_size, gpu

# å¯åŠ¨å‘½ä»¤:
# torchrun --nproc_per_node=2 multi_gpu_train.py
```

## ğŸƒ æ¨¡å‹è®­ç»ƒ

### 1. å¿«é€ŸéªŒè¯è®­ç»ƒ
```bash
# å°è§„æ¨¡æµ‹è¯• (çº¦5-10åˆ†é’Ÿ)
python gpu_train.py

# è‡ªå®šä¹‰å‚æ•°è®­ç»ƒ
CUDA_VISIBLE_DEVICES=0 MAX_SAMPLES=100 MAX_EPOCHS=1 python gpu_train.py
```

### 2. å®Œæ•´è®­ç»ƒæµç¨‹
```bash
# å…¨é‡æ•°æ®è®­ç»ƒ (å¯èƒ½éœ€è¦å‡ å°æ—¶)
CUDA_VISIBLE_DEVICES=0 \
MAX_SAMPLES=5000 \
MAX_EPOCHS=5 \
BATCH_SIZE=8 \
LEARNING_RATE=1e-4 \
python gpu_train.py
```

### 3. å¤šGPUè®­ç»ƒ (å¦‚æœæœ‰å¤šä¸ªGPU)
```bash
# ä½¿ç”¨2ä¸ªGPU
CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node=2 multi_gpu_train.py

# ä½¿ç”¨4ä¸ªGPU
CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --nproc_per_node=4 multi_gpu_train.py
```

## âš¡ æ€§èƒ½ä¼˜åŒ–

### 1. å†…å­˜ä¼˜åŒ–é…ç½®
åœ¨è®­ç»ƒè„šæœ¬ä¸­æ·»åŠ :

```python
# GPUå†…å­˜ä¼˜åŒ–è®¾ç½®
torch.backends.cudnn.benchmark = True  # ä¼˜åŒ–å·ç§¯æ€§èƒ½
torch.backends.cuda.matmul.allow_tf32 = True  # å¯ç”¨TF32
torch.backends.cudnn.allow_tf32 = True

# æ¢¯åº¦ç´¯ç§¯ (å½“GPUå†…å­˜ä¸å¤Ÿæ—¶)
gradient_accumulation_steps = 4
effective_batch_size = batch_size * gradient_accumulation_steps
```

### 2. é‡åŒ–é…ç½®ä¼˜åŒ–
```python
# 4bité‡åŒ–é…ç½® (èŠ‚çœæ˜¾å­˜)
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,  # ä½¿ç”¨bfloat16æå‡æ€§èƒ½
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_quant_storage=torch.uint8
)
```

### 3. LoRAå‚æ•°ä¼˜åŒ–
```python
# é«˜æ€§èƒ½LoRAé…ç½®
lora_config = LoraConfig(
    r=16,  # å¢åŠ rankæå‡æ€§èƒ½
    lora_alpha=32,
    target_modules=[
        "q_proj", "v_proj", "k_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj"
    ],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)
```

## ğŸ“Š ç›‘æ§å’ŒåŸºå‡†æµ‹è¯•

### 1. åˆ›å»ºGPUç›‘æ§è„šæœ¬
åˆ›å»º `monitor_gpu.py`:

```python
#!/usr/bin/env python3
"""GPUè®­ç»ƒç›‘æ§è„šæœ¬"""
import torch
import time
import psutil
from datetime import datetime

def monitor_training():
    """ç›‘æ§è®­ç»ƒè¿‡ç¨‹ä¸­çš„GPUä½¿ç”¨æƒ…å†µ"""
    while True:
        current_time = datetime.now().strftime("%H:%M:%S")
        
        # GPUä¿¡æ¯
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                gpu_memory_used = torch.cuda.memory_allocated(i) / 1e9
                gpu_memory_total = torch.cuda.get_device_properties(i).total_memory / 1e9
                gpu_utilization = gpu_memory_used / gpu_memory_total * 100
                
                print(f"[{current_time}] GPU {i}: {gpu_memory_used:.1f}GB/{gpu_memory_total:.1f}GB ({gpu_utilization:.1f}%)")
        
        # CPUå’Œå†…å­˜ä¿¡æ¯
        cpu_percent = psutil.cpu_percent()
        memory = psutil.virtual_memory()
        print(f"[{current_time}] CPU: {cpu_percent:.1f}%, Memory: {memory.percent:.1f}%")
        print("-" * 50)
        
        time.sleep(10)

if __name__ == "__main__":
    monitor_training()
```

### 2. åˆ›å»ºæ€§èƒ½åŸºå‡†æµ‹è¯•
åˆ›å»º `benchmark_gpu_model.py`:

```python
#!/usr/bin/env python3
"""GPUæ¨¡å‹æ€§èƒ½åŸºå‡†æµ‹è¯•"""
import torch
import time
from transformers import AutoTokenizer, AutoModelForCausalLM

def benchmark_inference():
    """åŸºå‡†æµ‹è¯•æ¨ç†æ€§èƒ½"""
    model_path = "./models/gpu_fine_tuned"
    
    # åŠ è½½æ¨¡å‹
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.float16,
        device_map="auto"
    )
    
    # æµ‹è¯•ç”¨ä¾‹
    test_queries = [
        "å¦‚ä½•é€€è´§ï¼Ÿ",
        "è¿™ä¸ªå•†å“æœ‰ä¿ä¿®å—ï¼Ÿ",
        "æ”¯æŒè´§åˆ°ä»˜æ¬¾å—ï¼Ÿ",
        "å¿«é€’å¤šä¹…èƒ½åˆ°ï¼Ÿ",
        "å¯ä»¥ä½¿ç”¨ä¼˜æƒ åˆ¸å—ï¼Ÿ"
    ]
    
    print("ğŸ§ª å¼€å§‹æ¨ç†æ€§èƒ½æµ‹è¯•...")
    
    total_time = 0
    for i, query in enumerate(test_queries):
        start_time = time.time()
        
        inputs = tokenizer(f"ç”¨æˆ·ï¼š{query}\nå®¢æœï¼š", return_tensors="pt")
        inputs = {k: v.to(model.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=100,
                temperature=0.1,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        end_time = time.time()
        
        inference_time = end_time - start_time
        total_time += inference_time
        
        print(f"Query {i+1}: {inference_time:.2f}s")
        print(f"Response: {response}")
        print("-" * 50)
    
    avg_time = total_time / len(test_queries)
    print(f"å¹³å‡æ¨ç†æ—¶é—´: {avg_time:.2f}s")
    print(f"ååé‡: {1/avg_time:.2f} queries/second")

if __name__ == "__main__":
    benchmark_inference()
```

## ğŸ› å¸¸è§é—®é¢˜

### 1. GPUå†…å­˜ä¸è¶³
```bash
# é”™è¯¯: CUDA out of memory
# è§£å†³æ–¹æ¡ˆ:
# 1. å‡å°‘batch_size
BATCH_SIZE=1 python gpu_train.py

# 2. å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
GRADIENT_CHECKPOINTING=True python gpu_train.py

# 3. ä½¿ç”¨4bité‡åŒ–
USE_QUANTIZATION=True python gpu_train.py
```

### 2. CUDAç‰ˆæœ¬ä¸åŒ¹é…
```bash
# æ£€æŸ¥CUDAç‰ˆæœ¬å…¼å®¹æ€§
python -c "import torch; print(torch.version.cuda)"
nvcc --version

# é‡æ–°å®‰è£…å¯¹åº”ç‰ˆæœ¬çš„PyTorch
pip uninstall torch torchvision torchaudio
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

### 3. å¤šGPUè®­ç»ƒé—®é¢˜
```bash
# æ£€æŸ¥åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ
python -c "import torch.distributed as dist; print('Distributed available')"

# è®¾ç½®æ­£ç¡®çš„ç¯å¢ƒå˜é‡
export MASTER_ADDR=localhost
export MASTER_PORT=12355
export WORLD_SIZE=2
export RANK=0
```

### 4. æ€§èƒ½ä¼˜åŒ–æ£€æŸ¥
```bash
# æ£€æŸ¥æ˜¯å¦å¯ç”¨äº†æ€§èƒ½ä¼˜åŒ–
python -c "
import torch
print(f'CUDNN Benchmark: {torch.backends.cudnn.benchmark}')
print(f'TF32 Matmul: {torch.backends.cuda.matmul.allow_tf32}')
print(f'TF32 CUDNN: {torch.backends.cudnn.allow_tf32}')
"
```

## ğŸ“ˆ æ¨èè®­ç»ƒé…ç½®

### å•GPUé…ç½® (RTX 3090/4090)
```bash
CUDA_VISIBLE_DEVICES=0 \
MAX_SAMPLES=1000 \
MAX_EPOCHS=3 \
BATCH_SIZE=4 \
LEARNING_RATE=2e-4 \
USE_QUANTIZATION=True \
python gpu_train.py
```

### é«˜ç«¯GPUé…ç½® (A100/V100)
```bash
CUDA_VISIBLE_DEVICES=0 \
MAX_SAMPLES=5000 \
MAX_EPOCHS=5 \
BATCH_SIZE=8 \
LEARNING_RATE=1e-4 \
USE_QUANTIZATION=False \
python gpu_train.py
```

### å¤šGPUé…ç½®
```bash
CUDA_VISIBLE_DEVICES=0,1,2,3 \
torchrun --nproc_per_node=4 \
multi_gpu_train.py \
--max_samples=10000 \
--epochs=5 \
--batch_size=16
```

---

## ğŸ¯ å¿«é€Ÿå¼€å§‹

1. **éªŒè¯ç¯å¢ƒ**: `python -c "import torch; print(torch.cuda.is_available())"`
2. **å®‰è£…ä¾èµ–**: `pip install -r requirements.txt`
3. **å¿«é€Ÿè®­ç»ƒ**: `python gpu_train.py`
4. **æµ‹è¯•æ¨¡å‹**: `python simple_gradio_test.py`
5. **æ€§èƒ½åŸºå‡†**: `python benchmark_gpu_model.py`

**é¢„è®¡è®­ç»ƒæ—¶é—´**:
- å¿«é€Ÿæµ‹è¯• (100æ ·æœ¬): 5-10åˆ†é’Ÿ
- æ ‡å‡†è®­ç»ƒ (1000æ ·æœ¬): 30-60åˆ†é’Ÿ  
- å®Œæ•´è®­ç»ƒ (5000æ ·æœ¬): 2-4å°æ—¶

**å†…å­˜éœ€æ±‚**:
- 4bité‡åŒ–: 8-12GB GPUå†…å­˜
- 16bitåŠç²¾åº¦: 16-20GB GPUå†…å­˜
- 32bitå…¨ç²¾åº¦: 28-32GB GPUå†…å­˜